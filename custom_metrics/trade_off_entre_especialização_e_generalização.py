# -*- coding: utf-8 -*-
"""Trade-off entre Especializa√ß√£o e Generaliza√ß√£o.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kQ9LtfoFkZ75QoUYkH6WyM55nPfIS3yL
"""



"""# An√°lise Quantitativa do Trade-off: Especializa√ß√£o vs. Generaliza√ß√£o em LLMs

Este notebook implementa um pipeline completo para avaliar o trade-off entre especializar um Modelo de Linguagem de Grande Porte (LLM) para uma tarefa espec√≠fica (Text-to-SQL) e a consequente perda de capacidade em tarefas de conhecimento geral.

**O processo consiste em:**
1.  **Configura√ß√£o:** Instala√ß√£o de bibliotecas e defini√ß√£o de par√¢metros.
2.  **Prepara√ß√£o de Dados:** Download e formata√ß√£o dos datasets Spider e MMLU.
3.  **M√©trica Customizada:** Defini√ß√£o e teste de uma m√©trica para avaliar a execu√ß√£o correta de queries SQL.
4.  **Avalia√ß√£o Baseline:** Medi√ß√£o do desempenho do modelo original, sem fine-tuning.
5.  **Fine-Tuning:** Especializa√ß√£o do modelo na tarefa Text-to-SQL usando a t√©cnica LoRA.
6.  **Avalia√ß√£o P√≥s-Treino:** Medi√ß√£o do desempenho do modelo especializado para quantificar o ganho na tarefa e a perda em generaliza√ß√£o.
7.  **An√°lise Final:** Compara√ß√£o dos resultados e discuss√£o do trade-off.
"""

!pip install -q transformers==4.41.2 torch==2.3.0 accelerate==0.30.1 bitsandbytes==0.43.1 datasets==2.19.1 peft==0.10.0 trl==0.8.6 deepeval==0.21.12 einops==0.8.0

from google.colab import drive
drive.mount('/content/drive')

from google.colab import drive
drive.mount('/content/drive')

# C√©lula de C√≥digo
import os
import torch
import json
import sqlite3
import pandas as pd
from getpass import getpass

from huggingface_hub import login
from google.colab import userdata


try:
    hf_token = userdata.get('HF_TOKEN')
    login(token=hf_token)
    print("‚úÖ Autenticado no Hugging Face com sucesso!")
except Exception as e:
    print("üîë Token do Hugging Face n√£o encontrado nos Secrets.")
    print("Por favor, insira seu token de leitura do Hugging Face:")
    hf_token = getpass()
    login(token=hf_token)

# Importa√ß√µes principais
from datasets import load_dataset, concatenate_datasets
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    set_seed,
    BitsAndBytesConfig
)
from peft import LoraConfig, PeftModel
from trl import SFTTrainer
from deepeval.metrics import BaseMetric
from deepeval.test_case import LLMTestCase

if torch.cuda.is_available():
    print(f"‚úÖ GPU detectada: {torch.cuda.get_device_name(0)}")
else:
    print("‚ö†Ô∏è Nenhuma GPU detectada. O treinamento ser√° extremamente lento.")


SEED = 42
set_seed(SEED)
MODEL_ID = "Qwen/Qwen2-1.5B-Instruct"


SPIDER_BASE_PATH = '/content/drive/MyDrive/spider_data'
SPIDER_DB_DIR = os.path.join(SPIDER_BASE_PATH, 'database')
SPIDER_TRAIN_FILE = os.path.join(SPIDER_BASE_PATH, 'train_spider.json')
SPIDER_DEV_FILE = os.path.join(SPIDER_BASE_PATH, 'dev.json')
SPIDER_TABLES_FILE = os.path.join(SPIDER_BASE_PATH, 'tables.json')


MMLU_SAMPLES = 50
MMLU_SUBCATEGORIES = {
    "stem": "college_computer_science",
    "humanities": "philosophy",
    "social_sciences": "econometrics"
}
SPIDER_DEV_SAMPLES = 50

CHOSEN_CONFIG = 'config1'
LORA_CONFIGS = {
    "config1": {"r": 4, "lora_alpha": 8, "lora_dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "learning_rate": 2e-5, "num_train_epochs": 2, "output_dir": "./models/qwen2-1.5b-spider-config1"},
}
TRAIN_ARGS = {
    "per_device_train_batch_size": 2, "gradient_accumulation_steps": 4, "warmup_steps": 10, "weight_decay": 0.01, "logging_steps": 10,
    "bf16": torch.cuda.is_available() and torch.cuda.is_bf16_supported(),
    "fp16": torch.cuda.is_available() and not torch.cuda.is_bf16_supported(),
    "optim": "paged_adamw_8bit", "save_strategy": "epoch",
}

os.makedirs("./data", exist_ok=True)
os.makedirs("./models", exist_ok=True)
os.makedirs("./results", exist_ok=True)
for f_path in [SPIDER_BASE_PATH, SPIDER_DB_DIR, SPIDER_TRAIN_FILE, SPIDER_DEV_FILE, SPIDER_TABLES_FILE]:
    if not os.path.exists(f_path):
        raise FileNotFoundError(f"ERRO: O arquivo ou diret√≥rio n√£o foi encontrado em '{f_path}'.")

"""### Passo 1: Download e Prepara√ß√£o dos Datasets

Esta c√©lula ir√°:
1.  Baixar o dataset **Spider** (incluindo os bancos de dados).
2.  Formatar o `train split` para o fine-tuning.
3.  Preparar o `development split` para a avalia√ß√£o.
4.  Criar a su√≠te de avalia√ß√£o customizada do **MMLU**.
"""

import json
from datasets import Dataset, DatasetDict
from tqdm import tqdm

def format_text_to_sql_chat(example, tokenizer):
    """Fun√ß√£o para formatar o exemplo para o SFTTrainer (sem altera√ß√µes)."""
    system_prompt = "You are a powerful Text-to-SQL model. Your task is to generate a SQL query based on the provided database schema and a natural language question."
    schema_str = "\n".join([
        f"Table {tbl['table_name_original']}: {', '.join(map(str, tbl['column_names_original']))}"
        for tbl in example['tables_info']
    ])
    prompt = f"Database Schema:\n{schema_str}\n\nQuestion:\n{example['question']}"
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": prompt},
        {"role": "assistant", "content": example["query"]},
    ]
    return {"text": tokenizer.apply_chat_template(messages, tokenize=False)}

def prepare_spider_dataset_from_drive(tokenizer):
    """
    Carrega os dados, e reconstr√≥i manualmente o campo 'tables_info' para garantir
    uma estrutura limpa e consistente antes de converter para o formato Dataset.
    """

    with open(SPIDER_TRAIN_FILE, 'r') as f:
        train_data_raw = json.load(f)
    with open(SPIDER_DEV_FILE, 'r') as f:
        dev_data_raw = json.load(f)
    with open(SPIDER_TABLES_FILE, 'r') as f:
        tables_data = json.load(f)

    schemas_dict = {db['db_id']: db for db in tables_data}

    def merge_and_reconstruct(data_list, schemas):
        clean_data = []
        print(f"Reconstruindo {len(data_list)} exemplos...")
        for item in tqdm(data_list):
            db_id = item['db_id']
            if db_id in schemas:
                schema_info = schemas[db_id]

                # ‚úÖ Corre√ß√£o definitiva:
                reconstructed_tables_info = []
                for i in range(len(schema_info['table_names_original'])):
                    # Para cada tabela, pegar s√≥ as colunas cujo √≠ndice corresponda a ela
                    columns_for_table = [
                        col[1] for col in schema_info['column_names_original'] if col[0] == i
                    ]
                    reconstructed_tables_info.append({
                        'table_name_original': schema_info['table_names_original'][i],
                        'column_names_original': columns_for_table
                    })

                new_clean_item = {
                    'db_id': db_id,
                    'question': item['question'],
                    'query': item['query'],
                    'tables_info': reconstructed_tables_info
                }
                clean_data.append(new_clean_item)
        return clean_data

    train_data_clean = merge_and_reconstruct(train_data_raw, schemas_dict)
    dev_data_clean = merge_and_reconstruct(dev_data_raw, schemas_dict)

    train_dataset = Dataset.from_list(train_data_clean)
    dev_dataset = Dataset.from_list(dev_data_clean)

    dataset = DatasetDict({'train': train_dataset, 'validation': dev_dataset})

    formatted_dataset = dataset['train'].map(
        lambda example: format_text_to_sql_chat(example, tokenizer),
        batched=False
    )
    formatted_dataset.to_json("./data/spider_train_formatted.json", orient="records", lines=True)

    return list(dev_dataset)

def prepare_mmlu_dataset():
    """Cria a su√≠te de avalia√ß√£o do MMLU (sem altera√ß√µes)."""
    print("Criando a su√≠te de avalia√ß√£o MMLU...")
    subsets = []
    for key, subcat_name in MMLU_SUBCATEGORIES.items():
        subset = load_dataset("cais/mmlu", subcat_name, split="test")
        sampled_subset = subset.shuffle(seed=SEED).select(range(MMLU_SAMPLES))
        sampled_subset = sampled_subset.map(lambda x: {"main_category": key})
        subsets.append(sampled_subset)
    eval_suite = concatenate_datasets(subsets)
    return list(eval_suite)

tokenizer_for_prep = AutoTokenizer.from_pretrained(MODEL_ID)

spider_dev_data = prepare_spider_dataset_from_drive(tokenizer_for_prep)
mmlu_eval_data = prepare_mmlu_dataset()

"""### Passo 2: Defini√ß√£o da M√©trica de Avalia√ß√£o `ExecutionAccuracy`

Aqui definimos a classe Python que ir√° se conectar ao banco de dados SQLite (localizado no seu Drive), executar as queries e comparar os resultados.
"""

class ExecutionAccuracyMetric(BaseMetric):
    def __init__(self, db_dir: str):
        if not os.path.isdir(db_dir):
            raise ValueError(f"O diret√≥rio do banco de dados '{db_dir}' n√£o existe.")
        self.db_dir = db_dir
        self.threshold = 1.0

    def measure(self, test_case: LLMTestCase) -> float:
        db_id = next((ctx.split(':')[1] for ctx in test_case.context if ctx.startswith("db_id:")), None)
        if not db_id:
            raise ValueError("O contexto do test_case deve conter 'db_id:<id>'")

        db_path = os.path.join(self.db_dir, db_id, f"{db_id}.sqlite")
        if not os.path.exists(db_path):
            self.reason = f"Falha: Banco de dados n√£o encontrado em '{db_path}'."
            self.score = 0.0
            return self.score

        try:
            actual_results = self._execute_query(db_path, test_case.actual_output)
        except Exception as e:
            self.reason = f"Erro ao executar a consulta gerada: {e}"
            self.score = 0.0
            return self.score

        try:
            expected_results = self._execute_query(db_path, test_case.expected_output)
        except Exception as e:
            self.reason = f"Erro ao executar a consulta ground-truth: {e}"
            self.score = 0.0
            return self.score

        if set(actual_results) == set(expected_results):
            self.reason = "Sucesso: Resultados id√™nticos."
            self.score = 1.0
        else:
            self.reason = "Falha: Resultados diferentes."
            self.score = 0.0
        return self.score

    def _execute_query(self, db_path: str, query: str):
        with sqlite3.connect(db_path) as conn:
            return conn.cursor().execute(query).fetchall()

    async def a_measure(self, test_case: LLMTestCase, **kwargs) -> float:
        return self.measure(test_case)

    def is_successful(self) -> bool:
        return self.score >= self.threshold

"""### Passo 3: Avalia√ß√£o Baseline (Desempenho do Modelo Original)

Agora, vamos medir o desempenho do `Qwen 7b` original em ambas as tarefas.

1.  **Text-to-SQL (Spider dev):** Usando um prompt de *few-shot*.
2.  **Conhecimento Geral (MMLU):** Usando a abordagem padr√£o de 4-shot.

**Aten√ß√£o:** A avalia√ß√£o pode ser demorada. Por isso, usamos um subconjunto dos dados (`SPIDER_DEV_SAMPLES`).
"""

# C√©lula de C√≥digo
from tqdm import tqdm
import numpy as np

# --- Carregar Modelo e Tokenizer para Avalia√ß√£o ---
print("Carregando modelo base para avalia√ß√£o...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    device_map="auto",
    torch_dtype=torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float32,
)
# Configurar pad token se n√£o existir
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
print("‚úÖ Modelo base carregado.")


print(f"\nIniciando avalia√ß√£o baseline Text-to-SQL em {SPIDER_DEV_SAMPLES} amostras...")
FEW_SHOT_PROMPT = """You are a powerful Text-to-SQL model. Your task is to generate a SQL query based on the provided database schema and a natural language question.

-- Example 1
Database Schema:
Table department: T1, department_id, name, creation, ranking, budget_in_billions, num_employees
Table head: T2, head_id, name, born_state, age
Table management: T3, department_id, head_id, temporary_acting
Question:
How many departments are there?
SQL Query:
SELECT count(*) FROM department

-- Example 2
Database Schema:
Table Highschooler: T1, ID, name, grade
Table Friend: T2, student_id, friend_id
Table Likes: T3, student_id, liked_id
Question:
Find the names of all students who are friends with someone named Gabriel.
SQL Query:
SELECT T1.name FROM Highschooler AS T1 JOIN Friend AS T2 ON T1.ID  =  T2.student_id JOIN Highschooler AS T3 ON T2.friend_id  =  T3.ID WHERE T3.name  =  'Gabriel'

-- Example 3
Database Schema:
Table body: T1, Body_ID, Body_Name, Body_Type
Table head: T2, Head_ID, Official_Name, Born_Year, Ground_Truth_Rank
Table in_orbit: T3, Body_ID, Head_ID, Period_days
Question:
What are the official names of the heads of bodies of type 'Planet'?
SQL Query:
SELECT T2.Official_Name FROM body AS T1 JOIN in_orbit AS T3 ON T1.Body_ID  =  T3.Body_ID JOIN head AS T2 ON T3.Head_ID  =  T2.Head_ID WHERE T1.Body_Type  =  'Planet'

-- New Task
Database Schema:
{schema}

Question:
{question}
SQL Query:
"""
sql_metric = ExecutionAccuracyMetric(db_dir=SPIDER_DB_DIR)
baseline_sql_scores = []

for item in tqdm(spider_dev_data[:SPIDER_DEV_SAMPLES]):
    schema_str = "\n".join([f"Table {tbl['table_name_original']}: {', '.join(tbl['column_names_original'])}" for tbl in item['tables_info']])
    prompt = FEW_SHOT_PROMPT.format(schema=schema_str, question=item['question'])

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=128, eos_token_id=tokenizer.eos_token_id)
    generated_sql = tokenizer.decode(outputs[0], skip_special_tokens=True).split("SQL Query:")[-1].strip()

    test_case = LLMTestCase(
        input=prompt,
        actual_output=generated_sql,
        expected_output=item['query'],
        context=[f"db_id:{item['db_id']}"]
    )
    sql_metric.measure(test_case)
    baseline_sql_scores.append(sql_metric.score)

baseline_sql_accuracy = np.mean(baseline_sql_scores)
print(f"üéØ Acur√°cia de Execu√ß√£o Baseline (Text-to-SQL): {baseline_sql_accuracy:.2%}")

print("\nIniciando avalia√ß√£o baseline MMLU...")
baseline_mmlu_results = {"overall": [], "stem": [], "humanities": [], "social_sciences": []}
for item in tqdm(mmlu_eval_data):
    question = item['question']
    choices = item['choices']
    answer_idx = item['answer']
    category = item['main_category']

    prompt = f"The following are multiple choice questions (with answers) about {category.replace('_', ' ')}.\n\n"
    prompt += f"Question: {question}\n"
    prompt += "Choices:\n"
    for i, choice in enumerate(choices):
        prompt += f"{chr(65+i)}. {choice}\n"
    prompt += "Answer:"

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=5)
    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()[-1] # Pega a √∫ltima letra

    correct = (prediction.upper() == chr(65 + answer_idx))
    baseline_mmlu_results['overall'].append(correct)
    baseline_mmlu_results[category].append(correct)

baseline_mmlu_accuracy = {cat: np.mean(scores) for cat, scores in baseline_mmlu_results.items()}
print(f"üß† Acur√°cia Baseline (MMLU Overall): {baseline_mmlu_accuracy['overall']:.2%}")
for cat in ["stem", "humanities", "social_sciences"]:
    print(f"   - {cat.capitalize()}: {baseline_mmlu_accuracy[cat]:.2%}")

del model
torch.cuda.empty_cache()

"""### Passo 4: Fine-Tuning com LoRA

Esta √© a fase de treinamento. Vamos carregar o modelo base em 4-bit (usando `bitsandbytes` para economizar mem√≥ria), aplicar os adaptadores LoRA e iniciar o treinamento usando o `SFTTrainer` da biblioteca `trl`.


"""

lora_params = LORA_CONFIGS[CHOSEN_CONFIG]
print(f"--- Iniciando Fine-Tuning com a configura√ß√£o: {CHOSEN_CONFIG} ---")
print(f"Par√¢metros: {lora_params}")

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    quantization_config=quantization_config,
    device_map="auto",
)
model.config.use_cache = False

tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

peft_config = LoraConfig(
    r=lora_params['r'],
    lora_alpha=lora_params['lora_alpha'],
    lora_dropout=lora_params['lora_dropout'],
    target_modules=lora_params['target_modules'],
    task_type="CAUSAL_LM",
)

train_dataset = load_dataset("json", data_files="./data/spider_train_formatted.json", split="train")

training_args = TrainingArguments(
    output_dir=lora_params['output_dir'],
    num_train_epochs=lora_params['num_train_epochs'],
    learning_rate=lora_params['learning_rate'],
    gradient_checkpointing=True,
    report_to="none",

    **TRAIN_ARGS
)

trainer = SFTTrainer(
    model=model,
    train_dataset=train_dataset,
    peft_config=peft_config,
    dataset_text_field="text",
    max_seq_length=64,
    tokenizer=tokenizer,
    args=training_args,
)

print("\nüöÄ Iniciando o treinamento LoRA com otimiza√ß√µes de mem√≥ria...")
trainer.train()
print("‚úÖ Treinamento conclu√≠do.")

final_adapter_path = os.path.join(lora_params['output_dir'], "final_adapter")
trainer.save_model(final_adapter_path)
print(f"‚úÖ Adaptador LoRA salvo em: {final_adapter_path}")

del model, trainer
torch.cuda.empty_cache()

"""### Passo 5: Avalia√ß√£o P√≥s-Fine-Tuning

Agora, carregamos o modelo base novamente, mas desta vez aplicamos os adaptadores LoRA que acabamos de treinar. Em seguida, executamos as mesmas avalia√ß√µes de antes para medir o novo desempenho.

-   Para a tarefa Text-to-SQL, **n√£o** usamos mais o prompt de few-shot, pois o modelo agora est√° especializado.
-   A queda na performance do MMLU indicar√° o "esquecimento catastr√≥fico".
"""

from tqdm import tqdm
import numpy as np

print("Carregando modelo fine-tuned para avalia√ß√£o...")
base_model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    device_map="auto",
    torch_dtype=torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float32,
)
adapter_path = os.path.join(LORA_CONFIGS[CHOSEN_CONFIG]['output_dir'], "final_adapter")
ft_model = PeftModel.from_pretrained(base_model, adapter_path)
ft_model = ft_model.merge_and_unload()

tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
print("‚úÖ Modelo fine-tuned carregado.")


print(f"\nIniciando avalia√ß√£o p√≥s-FT Text-to-SQL em {SPIDER_DEV_SAMPLES} amostras...")
ft_sql_scores = []
system_prompt_ft = "You are a powerful Text-to-SQL model. Your task is to generate a SQL query based on the provided database schema and a natural language question."

for item in tqdm(spider_dev_data[:SPIDER_DEV_SAMPLES]):
    schema_str = "\n".join([f"Table {tbl['table_name_original']}: {', '.join(tbl['column_names_original'])}" for tbl in item['tables_info']])
    prompt_str = f"Database Schema:\n{schema_str}\n\nQuestion:\n{item['question']}"
    messages = [
        {"role": "system", "content": system_prompt_ft},
        {"role": "user", "content": prompt_str},
    ]
    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

    inputs = tokenizer(prompt, return_tensors="pt").to(ft_model.device)
    outputs = ft_model.generate(**inputs, max_new_tokens=128, eos_token_id=tokenizer.eos_token_id)
    generated_sql = tokenizer.decode(outputs[0][len(inputs["input_ids"][0]):], skip_special_tokens=True).strip()

    test_case = LLMTestCase(
        input=prompt,
        actual_output=generated_sql,
        expected_output=item['query'],
        context=[f"db_id:{item['db_id']}"]
    )
    sql_metric.measure(test_case)
    ft_sql_scores.append(sql_metric.score)

ft_sql_accuracy = np.mean(ft_sql_scores)
print(f"üéØ Acur√°cia de Execu√ß√£o P√≥s-FT (Text-to-SQL): {ft_sql_accuracy:.2%}")

print("\nIniciando avalia√ß√£o p√≥s-FT MMLU (Regress√£o de Capacidade)...")
ft_mmlu_results = {"overall": [], "stem": [], "humanities": [], "social_sciences": []}
for item in tqdm(mmlu_eval_data):
    question, choices, answer_idx, category = item['question'], item['choices'], item['answer'], item['main_category']
    prompt = f"The following are multiple choice questions (with answers) about {category.replace('_', ' ')}.\n\nQuestion: {question}\nChoices:\n"
    for i, choice in enumerate(choices): prompt += f"{chr(65+i)}. {choice}\n"
    prompt += "Answer:"

    inputs = tokenizer(prompt, return_tensors="pt").to(ft_model.device)
    outputs = ft_model.generate(**inputs, max_new_tokens=5)
    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()[-1]

    correct = (prediction.upper() == chr(65 + answer_idx))
    ft_mmlu_results['overall'].append(correct)
    ft_mmlu_results[category].append(correct)

ft_mmlu_accuracy = {cat: np.mean(scores) for cat, scores in ft_mmlu_results.items()}
print(f"üß† Acur√°cia P√≥s-FT (MMLU Overall): {ft_mmlu_accuracy['overall']:.2%}")
for cat in ["stem", "humanities", "social_sciences"]:
    print(f"   - {cat.capitalize()}: {ft_mmlu_accuracy[cat]:.2%}")

del ft_model, base_model
torch.cuda.empty_cache()

"""### Passo 6: An√°lise Final do Trade-off

Finalmente, compilamos todos os resultados em uma tabela para comparar diretamente o desempenho antes e depois do fine-tuning. Calculamos a varia√ß√£o percentual para quantificar o ganho de especializa√ß√£o e a perda de generaliza√ß√£o.
"""

data = {
    "M√©trica": [
        "Text-to-SQL (Execution Accuracy)",
        "MMLU - Overall",
        "MMLU - STEM",
        "MMLU - Humanidades",
        "MMLU - Ci√™ncias Sociais"
    ],
    "Baseline": [
        baseline_sql_accuracy,
        baseline_mmlu_accuracy['overall'],
        baseline_mmlu_accuracy['stem'],
        baseline_mmlu_accuracy['humanities'],
        baseline_mmlu_accuracy['social_sciences']
    ],
    "Fine-Tuned": [
        ft_sql_accuracy,
        ft_mmlu_accuracy['overall'],
        ft_mmlu_accuracy['stem'],
        ft_mmlu_accuracy['humanities'],
        ft_mmlu_accuracy['social_sciences']
    ]
}

df_results = pd.DataFrame(data)

df_results['Varia√ß√£o (%)'] = ((df_results['Fine-Tuned'] - df_results['Baseline']) / df_results['Baseline']) * 100

df_results_styled = df_results.style.format({
    'Baseline': '{:.2%}',
    'Fine-Tuned': '{:.2%}',
    'Varia√ß√£o (%)': '{:+.2f}%'
}).applymap(
    lambda val: 'color: green' if val > 0 else 'color: red', subset=['Varia√ß√£o (%)']
).set_caption(f"Comparativo de Desempenho (Config: {CHOSEN_CONFIG})")

display(df_results_styled)

print("\n--- Discuss√£o dos Resultados ---\n")
gain_sql = df_results.loc[0, 'Varia√ß√£o (%)']
loss_mmlu = df_results.loc[1, 'Varia√ß√£o (%)']

print(f"O fine-tuning resultou em um ganho de performance de {gain_sql:+.2f}% na tarefa alvo de Text-to-SQL.")
print(f"Contudo, observamos uma perda de capacidade geral (MMLU Overall) de {loss_mmlu:+.2f}%.")

loss_humanities = df_results.loc[3, 'Varia√ß√£o (%)']
loss_stem = df_results.loc[2, 'Varia√ß√£o (%)']

print("\nAn√°lise por Categoria:")
print(f"- A maior degrada√ß√£o ocorreu em Humanidades ({loss_humanities:+.2f}%).")
print(f"- A menor degrada√ß√£o (ou at√© mesmo um leve ganho, em alguns casos) foi em STEM ({loss_stem:+.2f}%).")
print("\nHip√≥tese: O treinamento em SQL, uma linguagem l√≥gica e estruturada, pode ter refor√ßado caminhos neurais relacionados ao racioc√≠nio em STEM, enquanto negligenciou ou sobrescreveu os conhecimentos mais abstratos de humanidades. Este fen√¥meno √© um exemplo cl√°ssico do trade-off de especializa√ß√£o.")
print("\nImplica√ß√£o Pr√°tica: Para desenvolver LLMs comerciais especializados, √© crucial n√£o apenas medir o ganho na tarefa-alvo, mas tamb√©m monitorar a degrada√ß√£o em dom√≠nios importantes para o caso de uso, a fim de evitar a cria√ß√£o de um modelo 'superespecializado' e fr√°gil.")